{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e356be2",
   "metadata": {},
   "source": [
    "Reference: https://github.com/Forggtensky/Quantize_Pytorch_Vgg16AndMobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "439bb008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import torch.quantization\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "948671ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    1、Model architecture\n",
    "------------------------------\n",
    "\"\"\"\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\n",
    "    \n",
    "    Args:\n",
    "        in_planes: number of channels in input image\n",
    "        out_planes: number of channels produced by convolution\n",
    "        stride: stride of the convolution. Default: 1\n",
    "        groups: Number of blocked connections from input channels to output channels. Default: 1\n",
    "        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
    "        \n",
    "    Returns:\n",
    "        Convoluted layer of kernel size=3, with specified out_planes\n",
    "    \n",
    "    \"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\n",
    "    \n",
    "    Args:\n",
    "        in_planes: number of channels in input image\n",
    "        out_planes: number of channels produced by convolution\n",
    "        stride: stride of the convolution. Default: 1\n",
    "        \n",
    "    Returns:\n",
    "        Convoluted layer of kernel size=1, with specified out_planes\n",
    "        \n",
    "    \"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None, quantize=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.quantize = quantize\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        # FloatFunction()\n",
    "        self.skip_add = nn.quantized.FloatFunctional()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        # Notice the addition operation in both scenarios\n",
    "        if self.quantize:\n",
    "            out = self.skip_add.add(out, identity)\n",
    "        else:\n",
    "            out += identity\n",
    "\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block=BasicBlock, layers=[2, 2, 2, 2], num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None, mnist=False, quantize=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.quantize = quantize\n",
    "        if mnist:\n",
    "            num_channels = 1\n",
    "        else:\n",
    "            num_channels = 3\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace \n",
    "            # the 2x2 stride with a dilated convolution instead.\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(num_channels, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer, quantize=self.quantize))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer, quantize=self.quantize))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # Input are quantized\n",
    "        if self.quantize:\n",
    "            x = self.quant(x)\n",
    "    \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Outputs are dequantized\n",
    "        if self.quantize:\n",
    "            x = self.dequant(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "         # See note [TorchScript super()]\n",
    "        return self._forward_impl(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04a60702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "------------------------------\n",
    "    2、Helper functions\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader, neval_batches):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            print('.', end = '')\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "\n",
    "def run_benchmark(model_file, img_loader):\n",
    "    elapsed = 0\n",
    "    model = torch.jit.load(model_file)\n",
    "    model.eval()\n",
    "    num_batches = 30\n",
    "    # Run the scripted model on a few batches of images\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        if i < num_batches:\n",
    "            start = time.time()\n",
    "            output = model(images)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end-start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n",
    "    return elapsed\n",
    "\n",
    "def load_model(model_file, quantize=False):\n",
    "#     model_name = \"resnet18\"\n",
    "    model = ResNet(num_classes=10, mnist=False, quantize=quantize)\n",
    "    # vgg(model_name=model_name,num_classes=1000,init_weights=False)\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    \"\"\" Train the model with given dataset\n",
    "    \n",
    "    Args:\n",
    "        args: args like log interval\n",
    "        model: ResNet model to train\n",
    "        device: CPU/GPU\n",
    "        train_loader: dataset iterator\n",
    "        optimizer: optimizer to update weights\n",
    "        epoch: number of epochs to train for\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(F.log_softmax(output, dim=-1), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % args[\"log_interval\"] == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ca3468",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.594474\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.327580\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.265280\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.341282\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.092845\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.241495\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 1.132555\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.891339\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.925984\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.015046\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 1.012455\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.791094\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.793233\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.704739\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.698564\n",
      "\n",
      " Before quantization: \n",
      " ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "Size of baseline model\n",
      "Size (MB): 44.803725\n",
      "...................................Evaluation accuracy on 8960 images, 73.00\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "------------------------------\n",
    "    3. Define dataset and data loaders\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "def prepare_data_loaders(num_workers=8,\n",
    "                           train_batch_size=128,\n",
    "                           eval_batch_size=256):\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                             std=(0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                             std=(0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.CIFAR10(root=\"data\",\n",
    "                                             train=True,\n",
    "                                             download=True,\n",
    "                                             transform=train_transform)\n",
    "    # We will use test set for validation and test in this project.\n",
    "    # Do not use test set for validation in practice!\n",
    "    test_set = torchvision.datasets.CIFAR10(root=\"data\",\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=test_transform)\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "                                               batch_size=train_batch_size,\n",
    "                                               sampler=train_sampler,\n",
    "                                               num_workers=num_workers)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "                                              batch_size=eval_batch_size,\n",
    "                                              sampler=test_sampler,\n",
    "                                              num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "torch.manual_seed(191009)\n",
    "\n",
    "# data_path = 'data/imagenet_1k'\n",
    "saved_model_dir = './model/'\n",
    "float_model_file = 'resnet18_pretrained_float.pth'\n",
    "scripted_float_model_file = 'resnet18_quantization_scripted.pth'\n",
    "scripted_default_quantized_model_file = 'resnet18_quantization_scripted_default_quantized.pth'\n",
    "scripted_optimal_quantized_model_file = 'resnet18_quantization_scripted_optimal_quantized.pth'\n",
    "\n",
    "train_batch_size = 128\n",
    "eval_batch_size = 256\n",
    "\n",
    "train_loader, test_loader = prepare_data_loaders(train_batch_size=train_batch_size,\n",
    "                                                     eval_batch_size=eval_batch_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train Start ------------------------------\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "# seed = 1\n",
    "log_interval = 500\n",
    "save_model = True\n",
    "no_cuda = False\n",
    "\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "# torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "model = ResNet(num_classes=10, mnist=False).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "args = {}\n",
    "args[\"log_interval\"] = log_interval\n",
    "# print(device)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(args, model, device, train_loader, optimizer, epoch)\n",
    "\n",
    "if (save_model):\n",
    "    torch.save(model.state_dict(),saved_model_dir+float_model_file)\n",
    "\n",
    "# Train End ------------------------------\n",
    "    \n",
    "float_model = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "\n",
    "print('\\n Before quantization: \\n',float_model)\n",
    "float_model.eval()\n",
    "\n",
    "num_eval_batches = 35\n",
    "\n",
    "print(\"Size of baseline model\")\n",
    "print_size_of_model(float_model)\n",
    "\n",
    "# to get a “baseline” accuracy, see the accuracy of our un-quantized model\n",
    "top1, top5 = evaluate(float_model, criterion, test_loader, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file) # save un_quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d278508a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "\n",
      "Post Training Quantization Prepare: Inserting Observers by Calibrate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/as6416/anaconda3/lib/python3.9/site-packages/torch/ao/quantization/observer.py:172: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........Calibrate done\n",
      "Post Training Quantization: Convert done\n",
      "\n",
      " After quantization: \n",
      " ResNet(\n",
      "  (conv1): QuantizedConv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.1361324042081833, zero_point=65, padding=(3, 3), bias=False)\n",
      "  (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.20117953419685364, zero_point=64, padding=(1, 1), bias=False)\n",
      "      (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.13787569105625153, zero_point=66, padding=(1, 1), bias=False)\n",
      "      (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): QFunctional(\n",
      "        scale=0.18503576517105103, zero_point=39\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.253894180059433, zero_point=59, padding=(1, 1), bias=False)\n",
      "      (bn1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.12895435094833374, zero_point=64, padding=(1, 1), bias=False)\n",
      "      (bn2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): QFunctional(\n",
      "        scale=0.1966494619846344, zero_point=40\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.21242018043994904, zero_point=64, padding=(1, 1), bias=False)\n",
      "      (bn1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.10406181961297989, zero_point=56, padding=(1, 1), bias=False)\n",
      "      (bn2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.20766045153141022, zero_point=57, bias=False)\n",
      "        (1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): QFunctional(\n",
      "        scale=0.180756077170372, zero_point=54\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.19300372898578644, zero_point=57, padding=(1, 1), bias=False)\n",
      "      (bn1): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.11754865199327469, zero_point=56, padding=(1, 1), bias=False)\n",
      "      (bn2): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): QFunctional(\n",
      "        scale=0.18244272470474243, zero_point=39\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.16061504185199738, zero_point=66, padding=(1, 1), bias=False)\n",
      "      (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.07234752178192139, zero_point=58, padding=(1, 1), bias=False)\n",
      "      (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.1912572979927063, zero_point=64, bias=False)\n",
      "        (1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): QFunctional(\n",
      "        scale=0.14488306641578674, zero_point=54\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.11034546047449112, zero_point=56, padding=(1, 1), bias=False)\n",
      "      (bn1): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.07442373037338257, zero_point=63, padding=(1, 1), bias=False)\n",
      "      (bn2): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): QFunctional(\n",
      "        scale=0.147960364818573, zero_point=39\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.09650499373674393, zero_point=62, padding=(1, 1), bias=False)\n",
      "      (bn1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.03656806796789169, zero_point=60, padding=(1, 1), bias=False)\n",
      "      (bn2): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.13432222604751587, zero_point=64, bias=False)\n",
      "        (1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): QFunctional(\n",
      "        scale=0.12961402535438538, zero_point=64\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.05638863891363144, zero_point=56, padding=(1, 1), bias=False)\n",
      "      (bn1): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.04524665325880051, zero_point=68, padding=(1, 1), bias=False)\n",
      "      (bn2): QuantizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): QFunctional(\n",
      "        scale=0.13439950346946716, zero_point=45\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): QuantizedLinear(in_features=512, out_features=10, scale=0.18857209384441376, zero_point=53, qscheme=torch.per_tensor_affine)\n",
      "  (quant): Quantize(scale=tensor([0.0375]), zero_point=tensor([57]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      ")\n",
      "Size of model after quantization\n",
      "Size (MB): 11.305877\n",
      "...................................Evaluation accuracy on 8960 images, 72.88\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "------------------------------\n",
    "    4. Post-training static quantization\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "num_calibration_batches = 10\n",
    "\n",
    "myModel = load_model(saved_model_dir + float_model_file, quantize=True).to('cpu')\n",
    "myModel.eval()\n",
    "\n",
    "# Specify quantization configuration\n",
    "# Start with simple min/max range estimation and per-tensor quantization of weights\n",
    "myModel.qconfig = torch.quantization.default_qconfig\n",
    "print(myModel.qconfig)\n",
    "torch.quantization.prepare(myModel, inplace=True)\n",
    "\n",
    "# Calibrate with the training set\n",
    "print('\\nPost Training Quantization Prepare: Inserting Observers by Calibrate')\n",
    "evaluate(myModel, criterion, train_loader, neval_batches=num_calibration_batches)\n",
    "print(\"Calibrate done\")\n",
    "\n",
    "# Convert to quantized model\n",
    "torch.quantization.convert(myModel, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "\n",
    "\n",
    "print('\\n After quantization: \\n',myModel)\n",
    "\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(myModel)\n",
    "\n",
    "top1, top5 = evaluate(myModel, criterion, test_loader, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(myModel), saved_model_dir + scripted_default_quantized_model_file) # save default_quantized model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee07a6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " optimal quantize config: \n",
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "..........Calibrate done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/as6416/anaconda3/lib/python3.9/site-packages/torch/ao/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1\n",
      "/home/as6416/anaconda3/lib/python3.9/site-packages/torch/ao/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Optimal Quantization: Convert done\n",
      "Size of model after optimal quantization\n",
      "Size (MB): 11.393601\n",
      "...................................Evaluation accuracy on 8960 images, 72.47\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    5. optimal\n",
    "    ·Quantizes weights on a per-channel basis\n",
    "    ·Uses a histogram observer that collects a histogram of activations and then picks quantization parameters\n",
    "    in an optimal manner.\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "per_channel_quantized_model = load_model(saved_model_dir + float_model_file, quantize=True)\n",
    "per_channel_quantized_model.eval()\n",
    "# per_channel_quantized_model.fuse_model() # VGG dont need fuse\n",
    "per_channel_quantized_model.qconfig = torch.quantization.get_default_qconfig('fbgemm') # set the quantize config\n",
    "print('\\n optimal quantize config: ')\n",
    "print(per_channel_quantized_model.qconfig)\n",
    "\n",
    "torch.quantization.prepare(per_channel_quantized_model, inplace=True) # execute the quantize config\n",
    "evaluate(per_channel_quantized_model,criterion, train_loader, num_calibration_batches) # calibrate\n",
    "print(\"Calibrate done\")\n",
    "\n",
    "torch.quantization.convert(per_channel_quantized_model, inplace=True) # convert to quantize model\n",
    "print('Post Training Optimal Quantization: Convert done')\n",
    "\n",
    "print(\"Size of model after optimal quantization\")\n",
    "print_size_of_model(per_channel_quantized_model)\n",
    "\n",
    "top1, top5 = evaluate(per_channel_quantized_model, criterion, test_loader, neval_batches=num_eval_batches) # test acc\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_optimal_quantized_model_file) # save quantized model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b769c8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference time compare: \n",
      "Elapsed time:   0 ms\n",
      "Elapsed time:   1 ms\n",
      "Elapsed time:   1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.446307897567749"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    6. compare performance\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nInference time compare: \")\n",
    "run_benchmark(saved_model_dir + scripted_float_model_file, test_loader)\n",
    "run_benchmark(saved_model_dir + scripted_default_quantized_model_file, test_loader)\n",
    "run_benchmark(saved_model_dir + scripted_optimal_quantized_model_file, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" you can compare the model's size/accuracy/inference time.\n",
    "    ----------------------------------------------------------------------------------------\n",
    "                    | origin model | default quantized model | optimal quantized model\n",
    "    model size:     |    44.8 MB   |         11.305 MB       |        11.394 MB\n",
    "    test accuracy:  |    73        |         72.88           |        72.47\n",
    "    inference time: |    0 ms      |         1 ms            |        1 ms\n",
    "    ---------------------------------------------------------------------------------------\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "711e4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import thop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc3dff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_macs_and_params(model, input_size, example_inputs=None):\n",
    "    if example_inputs is None:\n",
    "        example_inputs = torch.randn(*input_size)\n",
    "    macs, params = thop.profile(model, inputs=(example_inputs, ), verbose=False)\n",
    "    return macs, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9d31129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024.0, 0.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_macs_and_params(per_channel_quantized_model, (1,3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff1ae086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024.0, 0.0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_macs_and_params(myModel, (1,3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f205299b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37220352.0, 11181642.0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_macs_and_params(float_model, (1,3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2c879a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    params = sum([np.prod(p.size()) for p in model.parameters()])\n",
    "    print(\"Number of Parameters: %.1fM\"%(params/1e6))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2de9e755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 11.2M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11181642"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(float_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "22edf5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 0.0M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9600"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aaba27cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters: 0.0M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9600"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(per_channel_quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a18f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
